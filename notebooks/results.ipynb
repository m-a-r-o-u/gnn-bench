{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6f88cf",
   "metadata": {},
   "source": [
    "\n",
    "# GNN Benchmark Results Analysis\n",
    "\n",
    "This notebook loads the SQLite database containing all benchmark runs, displays the results in tabular form, and visualizes the key metrics (validation accuracy and throughput) as plots. Ensure that you have run the benchmarking suite already, and that `results/results.db` and the `results/plots/` folder exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a36150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the results database\n",
    "db_path = \"results/results.db\"\n",
    "\n",
    "# Connect to the database and load the runs table into a DataFrame\n",
    "conn = sqlite3.connect(db_path)\n",
    "df = pd.read_sql_query(\"SELECT * FROM runs ORDER BY timestamp DESC\", conn)\n",
    "conn.close()\n",
    "\n",
    "# Display the first few rows in a DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dbf16",
   "metadata": {},
   "source": [
    "\n",
    "The table above shows the first few rows of the `runs` data. The columns include:\n",
    "\n",
    "- **id**: Auto-incremented primary key.\n",
    "- **experiment_name**: Identifier of the experiment.\n",
    "- **dataset**, **model**, **epochs**, **batch_size**, **lr**, **hidden_dim**, **seed**, **world_size**, **rank**: Hyperparameters.\n",
    "- **final_train_loss**, **final_val_loss**, **final_val_acc**: Performance metrics.\n",
    "- **total_train_time**, **throughput**: Additional metrics.\n",
    "- **timestamp**: Time at which the run was logged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff550025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by model and world_size for accuracy plot\n",
    "grouped_acc = df.groupby(['model', 'world_size'])['final_val_acc'].mean().unstack(level=0)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for model in grouped_acc.columns:\n",
    "    plt.plot(grouped_acc.index, grouped_acc[model], marker='o', label=model)\n",
    "\n",
    "plt.xlabel(\"Number of GPUs (world_size)\")\n",
    "plt.ylabel(\"Average Final Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy vs. Number of GPUs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084bc82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by model and world_size for throughput plot\n",
    "grouped_thr = df.groupby(['model', 'world_size'])['throughput'].mean().unstack(level=0)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for model in grouped_thr.columns:\n",
    "    plt.plot(grouped_thr.index, grouped_thr[model], marker='o', label=model)\n",
    "\n",
    "plt.xlabel(\"Number of GPUs (world_size)\")\n",
    "plt.ylabel(\"Average Throughput (samples/sec)\")\n",
    "plt.title(\"Throughput vs. Number of GPUs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e7466",
   "metadata": {},
   "source": [
    "\n",
    "## Embedded Plot Images\n",
    "\n",
    "If you prefer to view the saved PNG files directly, they are located in `results/plots/`. Below are the embedded images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "img_dir = \"results/plots\"\n",
    "for filename in sorted(os.listdir(img_dir)):\n",
    "    if filename.endswith(\".png\"):\n",
    "        display(Image(os.path.join(img_dir, filename)))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
